{"cells":[{"metadata":{"collapsed":true},"cell_type":"markdown","source":"# Analyzing Biotechnology Related Companies in the Proximity of Aachen, Germany"},{"metadata":{},"cell_type":"markdown","source":"## 1) The description of the problem and a discussion of the background"},{"metadata":{},"cell_type":"markdown","source":"A contractor is trying to start their own business in the field of biotechnology. They are young entrepreneurs and have deep knowledge about their area but they do not know exactly where to setup their laboratory. They want to know the locations of the biotechnology related companies and possibly why they are settled in that area. My aim is to provide them sufficient information about the neighborhoods of Aachen and to simplify their decision process. I am going to analyse the geospatial data in the proximity of Aachen. Besides, I will show the distribution of the company locations and explore the data. "},{"metadata":{},"cell_type":"markdown","source":"**Lets importe some necessary libraries**"},{"metadata":{"trusted":true},"cell_type":"code","source":"import requests # library to handle requests\nimport pandas as pd # library for data analsysis\nimport numpy as np # library to handle data in a vectorized manner\nimport random # library for random number generation\n\n!pip install geopy\nfrom geopy.geocoders import Nominatim # module to convert an address into latitude and longitude values\n    \n# tranforming json file into a pandas dataframe library\nfrom pandas.io.json import json_normalize\n\n# !conda install -c conda-forge folium=0.5.0 --yes\n!pip install folium\nimport folium # plotting library\n\nprint('Folium installed')\nprint('Libraries imported.')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Define Foursquare Credentials and Version"},{"metadata":{"trusted":true},"cell_type":"code","source":"CLIENT_ID = 'V4JR2CSZPYFIZFTFZTFM2QWSBN35YW4WOEMW3BSKHW0TG1OK' # your Foursquare ID\nCLIENT_SECRET = 'WIVZ4KIPUMCLPLO1ZQ3TVS53TLC1S2B0O4DLZA3OCRUCL03A' # your Foursquare Secret\nVERSION = '20200315'\nLIMIT = 500\nprint('Your credentails:')\nprint('CLIENT_ID: ' + CLIENT_ID)\nprint('CLIENT_SECRET:' + CLIENT_SECRET)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Lets assume we live in Aachen, Germany and want to start a bussiness nearby. First let's start by converting the Aachen`s address to its latitude and longitude coordinates "},{"metadata":{"trusted":true},"cell_type":"code","source":"# In order to define an instance of the geocoder, we need to define a user_agent. We will name our agent foursquare_agent, as shown below.\n\naddress = 'Aachen, Germany'\n\ngeolocator = Nominatim(user_agent=\"foursquare_agent\")\nlocation = geolocator.geocode(address)\nlatitude = location.latitude\nlongitude = location.longitude\nprint('latitute and longitute of Aachen: {}, {}'.format(latitude, longitude))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2) A description of the data and how it will be used to solve the problem"},{"metadata":{},"cell_type":"markdown","source":"- The contractor wants to start business in the field of biotechnology in the proximity of Aachen. For this purpose, I am going to search the venues regarding \n        - genetics\n        - biology\n        - biotechnology\n        - bioscience\n        - DNA\n\n- I am going to introduce all venues within 100 km distance from Aachen. \n\n- I am going to obtain 5 data frames and then combine them together so that I can analyse easily."},{"metadata":{},"cell_type":"markdown","source":"### Lets start to search for a specific venue category"},{"metadata":{},"cell_type":"markdown","source":"let's define a query to search for \"genetics\" related venues that are within 100km metres from the Aachen."},{"metadata":{},"cell_type":"markdown","source":"### 2.1. Genetics related venues"},{"metadata":{"trusted":true},"cell_type":"code","source":"search_query = 'genetic'\nradius = 100000\nprint(search_query + ' .... OK!')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Define the corresponding URL"},{"metadata":{"trusted":true},"cell_type":"code","source":"url = 'https://api.foursquare.com/v2/venues/search?client_id={}&client_secret={}&ll={},{}&v={}&query={}&radius={}&limit={}'.format(CLIENT_ID, CLIENT_SECRET, latitude, longitude, VERSION, search_query, radius, LIMIT)\nurl","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Send the GET Request and examine the results"},{"metadata":{"trusted":true},"cell_type":"code","source":"results = requests.get(url).json()\n# results","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Get relevant part of JSON and transform it into a pandas dataframe"},{"metadata":{"trusted":true},"cell_type":"code","source":"# assign relevant part of JSON to venues\nvenues = results['response']['venues']\n\n# tranform venues into a dataframe\ndf_genetics = json_normalize(venues)\ndf_genetics.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2.2. Biology related venues"},{"metadata":{},"cell_type":"markdown","source":"We can continue the same procedure for the biology related venues"},{"metadata":{"trusted":true},"cell_type":"code","source":"search_query = 'biology'\nprint(search_query + ' .... OK!')\nurl = 'https://api.foursquare.com/v2/venues/search?client_id={}&client_secret={}&ll={},{}&v={}&query={}&radius={}&limit={}'.format(CLIENT_ID, CLIENT_SECRET, latitude, longitude, VERSION, search_query, radius, LIMIT)\n\nresults = requests.get(url).json()\n\n# assign relevant part of JSON to venues\nvenues = results['response']['venues']\n\n# tranform venues into a dataframe\ndf_biology = json_normalize(venues)\ndf_biology.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2.3. Biotechnology related venues"},{"metadata":{"trusted":true},"cell_type":"code","source":"search_query = 'biotechnology'\nprint(search_query + ' .... OK!')\n\nurl = 'https://api.foursquare.com/v2/venues/search?client_id={}&client_secret={}&ll={},{}&v={}&query={}&radius={}&limit={}'.format(CLIENT_ID, CLIENT_SECRET, latitude, longitude, VERSION, search_query, radius, LIMIT)\n\nresults = requests.get(url).json()\n\n# assign relevant part of JSON to venues\nvenues = results['response']['venues']\n\n# tranform venues into a dataframe\ndf_biotechnology = json_normalize(venues)\ndf_biotechnology.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**We see that there is no biotechnology related company in the proximity of Aachen**"},{"metadata":{},"cell_type":"markdown","source":"### 2.4. Bioscience related venues"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"search_query = 'bioscience'\nprint(search_query + ' .... OK!')\n\nurl = 'https://api.foursquare.com/v2/venues/search?client_id={}&client_secret={}&ll={},{}&v={}&query={}&radius={}&limit={}'.format(CLIENT_ID, CLIENT_SECRET, latitude, longitude, VERSION, search_query, radius, LIMIT)\n\nresults = requests.get(url).json()\n\n# assign relevant part of JSON to venues\nvenues = results['response']['venues']\n\n# tranform venues into a dataframe\ndf_bioscience = json_normalize(venues)\ndf_bioscience.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_bioscience.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2.5. DNA related venues"},{"metadata":{"trusted":true},"cell_type":"code","source":"search_query = 'DNA'\nprint(search_query + ' .... OK!')\n\nurl = 'https://api.foursquare.com/v2/venues/search?client_id={}&client_secret={}&ll={},{}&v={}&query={}&radius={}&limit={}'.format(CLIENT_ID, CLIENT_SECRET, latitude, longitude, VERSION, search_query, radius, LIMIT)\n\nresults = requests.get(url).json()\n\n# assign relevant part of JSON to venues\nvenues = results['response']['venues']\n\n# tranform venues into a dataframe\ndf_DNA = json_normalize(venues)\ndf_DNA.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2.6. Lets investigate columns of the 5 dataframes"},{"metadata":{"trusted":true},"cell_type":"code","source":"print ('Are all the columns of the df_genetics and df_biology the same? ', all(df_genetics.columns == df_biology.columns))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print ('Are all the columns of the df_bioscience and df_DNA the same? ', all(df_bioscience.columns == df_DNA.columns))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**We see that all columns of the dataframes are not the same. Lets depict their shapes**"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('shape of the dataframe genetics: ', df_genetics.shape)\nprint('shape of the dataframe biology: ', df_biology.shape)\nprint('shape of the dataframe bioscience: ', df_bioscience.shape)\nprint('shape of the dataframe DNA: ', df_DNA.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Now lets put lables of each dataframe so that we can use this information later on**"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_genetics['label'] = 'genetics'\ndf_biology['label'] = 'biology'\ndf_bioscience['label'] = 'bioscience'\ndf_DNA['label'] = 'DNA'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('shape of the dataframe genetics: ', df_genetics.shape)\nprint('shape of the dataframe biology: ', df_biology.shape)\nprint('shape of the dataframe bioscience: ', df_bioscience.shape)\nprint('shape of the dataframe DNA: ', df_DNA.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Lets, combine all dataframes and add a new feature called \"type\" as shown below**"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.concat([df_genetics, df_biology, df_bioscience, df_DNA], ignore_index=True, sort=False)\ndf['type'] = 'business'\ndf.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Now our dataframe is ready to for cleaning,  EDA and further analysis**\n\n**But before that, lets visualize these venues**"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"venues_map = folium.Map(location=[latitude, longitude], zoom_start=7) # generate map centred around Aachen\n\n# add a red circle marker to represent Aachen\nfolium.vector_layers.CircleMarker(\n    [latitude, longitude],\n    radius=10,\n    color='red',\n    popup='Aachen',\n    fill = True,\n    fill_color = 'red',\n    fill_opacity = 0.6\n).add_to(venues_map)\n\n# add the venues that are nearby as blue circle markers\nfor lat, lng, label in zip(df['location.lat'], df['location.lng'], df['label']):\n    folium.vector_layers.CircleMarker(\n        [lat, lng],\n        radius=5,\n        color='blue',\n        popup=label,\n        fill = True,\n        fill_color='blue',\n        fill_opacity=0.6\n    ).add_to(venues_map)\n\n# display map\nvenues_map","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**to see the plot, please refer to the png file named as \"biotechnology venues that are nearby aachen\" in the my coursera capstone repocitory**"},{"metadata":{},"cell_type":"markdown","source":"## How we are going to analyse the data?\n\nNow, we know the locations of nearby venues of Aachen. We have seen that there is no biotechnology venue within 100 km of Aachen. Moreover, there is no any venue in the southern part of the Aachen. On the other hand, there are 33 venues that are related to my contractors business. \n\nTherefore, we have to explore these venues more and learn how they are related with our business.\n\nWe will investigate why there is no any venues in the souther part of Aachen\n\nWe will increase the radius to see how the distribution of the other venues changes\n\nWe will also investigate posible target markets of our business such as \"genetic diagnostic centers\" or \"pharmaceutical companies\" and their distributions\n\nAnd finally, we will recommend some possible locations for their laboratory."},{"metadata":{},"cell_type":"markdown","source":"## 3) Biotechnology Related Venues that are within 50km from the Aachen"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# GENETICS\nsearch_query = 'genetic'\nradius = 50000\nprint(search_query + ' .... OK!')\nurl = 'https://api.foursquare.com/v2/venues/search?client_id={}&client_secret={}&ll={},{}&v={}&query={}&radius={}&limit={}'.format(CLIENT_ID, CLIENT_SECRET, latitude, longitude, VERSION, search_query, radius, LIMIT)\nresults = requests.get(url).json()\nvenues = results['response']['venues']\ndf_genetics_50 = json_normalize(venues)\n\n# BIOLOGY\nsearch_query = 'biology'\nprint(search_query + ' .... OK!')\nurl = 'https://api.foursquare.com/v2/venues/search?client_id={}&client_secret={}&ll={},{}&v={}&query={}&radius={}&limit={}'.format(CLIENT_ID, CLIENT_SECRET, latitude, longitude, VERSION, search_query, radius, LIMIT)\nresults = requests.get(url).json()\nvenues = results['response']['venues']\ndf_biology_50 = json_normalize(venues)\n\n# BIOTECHNOLOGY\nsearch_query = 'biotechnology'\nprint(search_query + ' .... OK!')\nurl = 'https://api.foursquare.com/v2/venues/search?client_id={}&client_secret={}&ll={},{}&v={}&query={}&radius={}&limit={}'.format(CLIENT_ID, CLIENT_SECRET, latitude, longitude, VERSION, search_query, radius, LIMIT)\nresults = requests.get(url).json()\nvenues = results['response']['venues']\ndf_biotechnology_50 = json_normalize(venues)\n\n# BIOSCIENCE\nsearch_query = 'bioscience'\nprint(search_query + ' .... OK!')\nurl = 'https://api.foursquare.com/v2/venues/search?client_id={}&client_secret={}&ll={},{}&v={}&query={}&radius={}&limit={}'.format(CLIENT_ID, CLIENT_SECRET, latitude, longitude, VERSION, search_query, radius, LIMIT)\nresults = requests.get(url).json()\nvenues = results['response']['venues']\ndf_bioscience_50 = json_normalize(venues)\n\n# DNA\nsearch_query = 'DNA'\nprint(search_query + ' .... OK!')\nurl = 'https://api.foursquare.com/v2/venues/search?client_id={}&client_secret={}&ll={},{}&v={}&query={}&radius={}&limit={}'.format(CLIENT_ID, CLIENT_SECRET, latitude, longitude, VERSION, search_query, radius, LIMIT)\nresults = requests.get(url).json()\nvenues = results['response']['venues']\ndf_DNA_50 = json_normalize(venues)\n\ndf_genetics_50['label'] = 'genetics'\ndf_biology_50['label'] = 'biology'\ndf_biotechnology_50['label'] = 'biotechnology'\ndf_bioscience_50['label'] = 'bioscience'\ndf_DNA_50['label'] = 'DNA'\n\ndf_50 = pd.concat([df_genetics_50, df_biology_50, df_biotechnology_50, df_bioscience_50, df_DNA_50], ignore_index=True, sort=False)\n\n# FOLIUM MAP FOR 200KM \nvenues_map_50 = folium.Map(location=[latitude, longitude], zoom_start=7) # generate map centred around Aachen\n# add a red circle marker to represent Aachen\nfolium.vector_layers.CircleMarker(\n    [latitude, longitude],\n    radius=10,\n    color='red',\n    popup='Aachen',\n    fill = True,\n    fill_color = 'red',\n    fill_opacity = 0.6\n).add_to(venues_map_50)\n# add the venues that are nearby as blue circle markers\nfor lat, lng, label in zip(df_50['location.lat'], df_50['location.lng'], df_50['label']):\n    folium.vector_layers.CircleMarker(\n        [lat, lng],\n        radius=5,\n        color='blue',\n        popup=label,\n        fill = True,\n        fill_color='blue',\n        fill_opacity=0.6\n    ).add_to(venues_map_50)\n# display map\nvenues_map_50","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"markdown","source":"**There are only 4 business venues within 50km**"},{"metadata":{},"cell_type":"markdown","source":"## 4) Target Markets of the Business"},{"metadata":{},"cell_type":"markdown","source":"### 4.1) Lets search for \"genetic diagnostic centers\", \"pharmaceutical companies\", \"genetic hospital\", \"biopharma\" and \"life science\" venues"},{"metadata":{"trusted":true},"cell_type":"code","source":"# genetic diagnostic centers\nsearch_query = 'genetic diagnos'\nradius = 100000\nprint(search_query + ' .... OK!')\nurl = 'https://api.foursquare.com/v2/venues/search?client_id={}&client_secret={}&ll={},{}&v={}&query={}&radius={}&limit={}'.format(CLIENT_ID, CLIENT_SECRET, latitude, longitude, VERSION, search_query, radius, LIMIT)\nresults = requests.get(url).json()\nvenues = results['response']['venues']\ndf_diagnos = json_normalize(venues)\n\n# pharmaceutical companies\nsearch_query = 'pharmaceutical'\nprint(search_query + ' .... OK!')\nurl = 'https://api.foursquare.com/v2/venues/search?client_id={}&client_secret={}&ll={},{}&v={}&query={}&radius={}&limit={}'.format(CLIENT_ID, CLIENT_SECRET, latitude, longitude, VERSION, search_query, radius, LIMIT)\nresults = requests.get(url).json()\nvenues = results['response']['venues']\ndf_pharmaceutical = json_normalize(venues)\n\n# genetic hospital\nsearch_query = 'genetic hospital'\nprint(search_query + ' .... OK!')\nurl = 'https://api.foursquare.com/v2/venues/search?client_id={}&client_secret={}&ll={},{}&v={}&query={}&radius={}&limit={}'.format(CLIENT_ID, CLIENT_SECRET, latitude, longitude, VERSION, search_query, radius, LIMIT)\nresults = requests.get(url).json()\nvenues = results['response']['venues']\ndf_hospital = json_normalize(venues)\n\n# biopharma\nsearch_query = 'biopharma'\nprint(search_query + ' .... OK!')\nurl = 'https://api.foursquare.com/v2/venues/search?client_id={}&client_secret={}&ll={},{}&v={}&query={}&radius={}&limit={}'.format(CLIENT_ID, CLIENT_SECRET, latitude, longitude, VERSION, search_query, radius, LIMIT)\nresults = requests.get(url).json()\nvenues = results['response']['venues']\ndf_biopharma = json_normalize(venues)\n\n# life science\nsearch_query = 'life science'\nprint(search_query + ' .... OK!')\nurl = 'https://api.foursquare.com/v2/venues/search?client_id={}&client_secret={}&ll={},{}&v={}&query={}&radius={}&limit={}'.format(CLIENT_ID, CLIENT_SECRET, latitude, longitude, VERSION, search_query, radius, LIMIT)\nresults = requests.get(url).json()\nvenues = results['response']['venues']\ndf_life_science = json_normalize(venues)\n\ndf_diagnos['label'] = 'genetic diagnosis'\ndf_pharmaceutical['label'] = 'pharmaceutical'\ndf_hospital['label'] = 'genetic hospital'\ndf_biopharma['label'] = 'biopharma'\ndf_life_science['label'] = 'life science'\n\ndf_market = pd.concat([df_diagnos, df_pharmaceutical, df_hospital, df_biopharma, df_life_science], ignore_index=True, sort=False)\ndf_market.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_market['type'] = 'target market'\ndf_market.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('shape of the taget market data: ', df_market.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4.2) Display Target Market Venues on the Map"},{"metadata":{"trusted":true},"cell_type":"code","source":"# FOLIUM MAP MARKET\nmarket_map = folium.Map(location=[latitude, longitude], zoom_start=8) # generate map centred around Aachen\n# add a red circle marker to represent Aachen\nfolium.vector_layers.CircleMarker(\n    [latitude, longitude],\n    radius=10,\n    color='red',\n    popup='Aachen',\n    fill = True,\n    fill_color = 'red',\n    fill_opacity = 0.6\n).add_to(market_map)\n\n# add the venues that are nearby as green circle markers\nfor lat, lng, label in zip(df_market['location.lat'], df_market['location.lng'], df_market['label']):\n    folium.vector_layers.CircleMarker(\n        [lat, lng],\n        radius=5,\n        color='green',\n        popup=label,\n        fill = True,\n        fill_color='green',\n        fill_opacity=0.6\n    ).add_to(market_map)\n# display map\nmarket_map","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4.3) Diplay Business and Target Market Venues on the same Map"},{"metadata":{"trusted":true},"cell_type":"code","source":"business_market_map = folium.Map(location=[latitude, longitude], zoom_start=7) # generate map centred around Aachen\n\n# add a red circle marker to represent Aachen\nfolium.vector_layers.CircleMarker(\n    [latitude, longitude],\n    radius=10,\n    color='red',\n    popup='Aachen',\n    fill = True,\n    fill_color = 'red',\n    fill_opacity = 0.6\n).add_to(business_market_map)\n\n# add the business venues that are nearby as blue circle markers\nfor lat, lng, label in zip(df['location.lat'], df['location.lng'], df['label']):\n    folium.vector_layers.CircleMarker(\n        [lat, lng],\n        radius=5,\n        color='blue',\n        popup=label,\n        fill = True,\n        fill_color='blue',\n        fill_opacity=0.6\n    ).add_to(business_market_map)\n\n# add the target market venues that are nearby as green circle markers\nfor lat, lng, label in zip(df_market['location.lat'], df_market['location.lng'], df_market['label']):\n    folium.vector_layers.CircleMarker(\n        [lat, lng],\n        radius=5,\n        color='green',\n        popup=label,\n        fill = True,\n        fill_color='green',\n        fill_opacity=0.6\n    ).add_to(business_market_map)\n    \n# display the map\nbusiness_market_map","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 5) Lets combine both dataframes"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('shape of the business venues dataframe: ', df.shape)\nprint('shape of the target market venues dataframe: ', df_market.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_all = pd.concat([df, df_market], ignore_index=True, sort=False)\nprint('shape of the business & target market venues dataframe: ', df_all.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 6) Data Wrangling\n0. Change the column names\n1. Check for missing data\n2. Check for data formats\n3. Data Standardization\n4. Data Normalization\n5. Binning\n6. Indicator variable (or dummy variable)"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_all.rename(columns={\"location.formattedAddress\": \"adress\", \"location.cc\": \"country_code\", \"location.distance\": \"distance (m)\", \"location.city\": \"city\",\n                       \"location.country\": \"country\", \"location.lat\": \"latitute\", \"location.lng\": \"longitute\", \"location.postalCode\": \"postcode\", \n                       \"location.state\": \"state\"}, inplace = True)\ndf_all.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 6.1) Lets first check our dataframe for missing data\n- 6.1.1 Check for 5, 10 and 20 rows\n- 6.1.2 To see which of the values are missing we can use .isnull() method. \n        It will return boolean values:\n        if the value is True, it means that value is missing\n        otherwise, it is not empty\n- 6.1.3 Count missing values "},{"metadata":{"trusted":true},"cell_type":"code","source":"n = 2\ndf_all.head(n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# lets create missing dataframe\nmissing_data = df_all.isnull()\nmissing_data.head()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# Count missing values in each column\nfor column in missing_data.columns.values.tolist():\n    print(column)\n    print (missing_data[column].value_counts())\n    print(\"\")    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Based on the summary above, each column has 197 rows of data, seven columns containing missing data\n\nThese are:\n\n- location.address:     63\n- city :       43\n- postcode:             77 \n- state:                45 \n- location.crossStreet:  191\n- venuePage.id:          190\n- location.neighborhood: 194"},{"metadata":{},"cell_type":"markdown","source":"<h3 id=\"deal_missing_values\">Deal with missing data</h3>\n\n<b>How to deal with missing data?</b>\n\n<ol>\n    <li>drop data<br>\n        a. drop the whole row<br>\n        b. drop the whole column\n    </li>\n    <li>replace data<br>\n        a. replace it by mean<br>\n        b. replace it by frequency<br>\n        c. replace it based on other functions\n    </li>\n</ol>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# we see all values of \"hasPerk\" is false, so we can drop this column since it does not give us any information\nprint('value counts of hasPerk: ', df_all.hasPerk.value_counts())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# \"location.address\" and \"adress\" columns are almost the same, so drop the \"location.address\"\n# We have longitute and latitute seperately so no need for 'location.labeledLatLngs'. Drop also this column","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lets see \"location.crossStreet\" which are not null (only 6 of them is not null)\n# we see that the information in this column is almost the same as the adress column, so we can safely drop also this column\ndf_all[df_all['location.crossStreet'].notnull()]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lets see \"venuePage.id\" which are not null (only 7 of them is not null)\n# We can safely drop also this column\ndf_all[df_all['venuePage.id'].notnull()]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lets see \"location.neighborhood\" which are not null (only 3 of them is not null)\n# We can safely drop also this column\ndf_all[df_all['location.neighborhood'].notnull()]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_new = df_all.drop(['hasPerk', 'location.address', 'location.labeledLatLngs', 'location.crossStreet', 'venuePage.id', 'location.neighborhood'], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_new.head(3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Lets investigate \"categories\" feature in more detail"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_new.categories[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# at the 20th and 150th rows, the categories feature is an empty list, but it is not null !!!\nprint('before')\nprint('-'*20)\nprint(df_new.categories[20])\nprint(df_new.categories[150])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# lets convert them to nan, which is a better way to represent them\n\nimport warnings \nwarnings.filterwarnings('ignore')\n\ndf_new.categories[20] = [{'id': np.nan,\n  'name': np.nan,\n  'pluralName': np.nan,\n  'shortName': np.nan,\n  'icon': {'prefix': np.nan,\n   'suffix': np.nan},\n  'primary': np.nan}]\n\ndf_new.categories[150] = [{'id': np.nan,\n  'name': np.nan,\n  'pluralName': np.nan,\n  'shortName': np.nan,\n  'icon': {'prefix': np.nan,\n   'suffix': np.nan},\n  'primary': np.nan}]\n\n# print filled values\nprint('after')\nprint('-'*20)\nprint(df_new.categories[20])\nprint(df_new.categories[150])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_new['category_name'] = np.nan\n\nfor i in range(0, len(df_new)):\n    df_new['category_name'][i] = df_new.iloc[i,2][0]['pluralName']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_new = df_new.drop('categories', axis = 1)\ndf_new.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Lets investigate \"city\" feature in more detail"},{"metadata":{"trusted":true},"cell_type":"code","source":"# we have 44 NaN values in city feature\nprint('Number of NaN values in city column: ', df_new.isnull()['city'].value_counts()[1])\nprint('The rest is as follows')\ndf_new.city.value_counts()[0:10]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**How to hande with missing data in cities?**\n\nfill null values with the most frequent value\n\n1. Germany\n\nWe see that in Germany; Köln, Bonn and Düsseldorf are the most frequent cities. Köln is in the center of these 3 cities and is the most frequent one\n\n2. Netherlands\n\nWe see that in Netherlands; Eindhoven and Maastricht are the most frequent cities.\n\n\n3. Belgium\n\nWe see that there is no frequent city\n\n**Since there is no very frequent city in the dataframe, we will choose randomly from the countries**\n\n**Although we fill them with rondom choise, the more frequent city is the more will be chosen**\n\n**For example, Köln is the most frequent city in Germany and thus most probably it will be used to fill cities in Germany more as compared to other cities**\n\n**This is valid also for Netherlands and Belgium**"},{"metadata":{"trusted":true},"cell_type":"code","source":"for country in ['DE', 'NL', 'BE']:\n    df_city_nul = df_new[df_new['city'].isnull()][df_new['country_code'] == country]\n    df_city_not_null = df_new[df_new['city'].notnull()][df_new['country_code'] == country]\n    cities = df_city_not_null['city'].tolist()\n\n    nul_city_rows = df_city_nul.index.tolist()\n    for row in nul_city_rows:\n        df_new.loc[row, 'city'] = random.choice(cities)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Now we have 0 NaN values in city feature\nprint('How many NaN values in city we have now? ', df_new['city'].isna().sum())\nprint('The rest is as follows')\ndf_new.city.value_counts()[0:10]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Lets investigate \"postcode\" feature in more detail"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('We have {} null values in postcode'.format(df_new.isnull()['postcode'].value_counts()[1]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since it does not give us much information and there are lots of null values, we will drop postcode feature. If we try to fill these values somehow, we may bias the results "},{"metadata":{"trusted":true},"cell_type":"code","source":"df_new.drop('postcode', axis = 1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Lets investigate \"state\" feature in more detail"},{"metadata":{"trusted":true},"cell_type":"code","source":"# if we know the the country, it is easy to guess the state like in Germany\n# thus we can fill null values easliy\nprint('States in Germany')\nstate_DE = df_new[df_new.country_code == 'DE']['state']\nprint(state_DE.value_counts())\n\nprint()\n\nprint('States in Netherlands')\nstate_NL = df_new[df_new.country_code == 'NL']['state']\nprint(state_NL.value_counts())\n\nprint()\n\nprint('States in Belgium')\nstate_NL = df_new[df_new.country_code == 'BE']['state']\nprint(state_NL.value_counts())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Similarly like we did in city feature:**"},{"metadata":{"trusted":true},"cell_type":"code","source":"for country in ['DE', 'NL', 'BE']:\n    df_state_nul = df_new[df_new['state'].isnull()][df_new['country_code'] == country]\n    df_state_not_null = df_new[df_new['state'].notnull()][df_new['country_code'] == country]\n    states = df_state_not_null['state'].tolist()\n\n    nul_state_rows = df_state_nul.index.tolist()\n    for row in nul_state_rows:\n        df_new.loc[row, 'state'] = random.choice(states)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Now we have 0 NaN values in state feature\nprint('How many NaN values in state we have now? ', df_new['state'].isna().sum())\nprint('The rest is as follows')\ndf_new.state.value_counts()[0:10]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Lets investigate \"referralId\" feature in more detail"},{"metadata":{"trusted":true},"cell_type":"code","source":"# check for the feature referralId\nprint('How many NaN values in referralId we have? ', df_new['referralId'].isna().sum())\nprint('We have: ')\ndf_new.referralId.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Lets check whether we have null values any more**"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_new.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have 2 null values in category_name\n\nThese are:"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_new[df_new.category_name.isnull()]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can safely drop these 2 rows"},{"metadata":{"trusted":true},"cell_type":"code","source":"# simply drop whole row with NaN in \"category_name\" column\ndf_new.dropna(subset=[\"category_name\"], axis=0, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets check again if we have null values"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_new.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_new.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Good! Now, we have obtained the dataset with no missing values.**"},{"metadata":{},"cell_type":"markdown","source":"### 6.2) Lets check for data formats "},{"metadata":{},"cell_type":"markdown","source":"<p>The last step in data cleaning is checking and making sure that all data is in the correct format (int, float, text or other).</p>\n\nIn Pandas, we use \n<p><b>.dtypes</b> to check the data type</p>\n<p><b>.astype()</b> to change the data type</p>"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_new.dtypes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Good!\n\n**All data in its proper format**"},{"metadata":{},"cell_type":"markdown","source":"### 6.3) Data Standardization"},{"metadata":{},"cell_type":"markdown","source":"<p>\nData is usually collected from different agencies with different formats.\n(Data Standardization is also a term for a particular type of data normalization, where we subtract the mean and divide by the standard deviation)\n</p>\n    \n<b>What is Standardization?</b>\n<p>Standardization is the process of transforming data into a common format which allows the researcher to make the meaningful comparison.\n</p>\n\n<b>Example</b>\n<p>Transform distance (m) to distance (km):</p>\n<p>In our dataset, the distance column is represented by m (meters). For convinience, we will convert this into km (kilometers)</p>\n<p>We will need to apply <b>data transformation</b> to distance(m) into distance(km)</p>\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Convert distance(m) into distance(km) dividing by 1000\ndf_new['distance (m)'] = (df_new['distance (m)']/1000).round(1)\n\n# rename the column\ndf_new.rename(columns = {'distance (m)': 'distance (km)'}, inplace = True)\n\n# check the transformed data \ndf_new.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**There is no other features that need standardization**"},{"metadata":{},"cell_type":"markdown","source":"### 6.4) Data Normalization"},{"metadata":{},"cell_type":"markdown","source":"<b>Why normalization?</b>\n<p>Normalization is the process of transforming values of several variables into a similar range. Typical normalizations include scaling the variable so the variable average is 0, scaling the variable so the variance is 1, or scaling variable so the variable values range from 0 to 1\n</p>\n\n<p>Let's normalize the columns \"distance (km)\", \"latitute\" and \"longitute\" </p>\n<p><b>Target:</b>would like to Normalize those variables so their value ranges from 0 to 1.</p>\n<p><b>Approach:</b> use MinMaxScaler</p>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# sclae 'distance (km)', 'latitute' and 'longitute' columns\n\nfrom sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\nk = df_new[['distance (km)', 'latitute', 'longitute']]\ndf_new[['distance (km)', 'latitute', 'longitute']] = scaler.fit_transform(k)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# convert 'city' and 'category_name' into numerical values using LabelEncoder\n\nfrom sklearn.preprocessing import LabelEncoder\nLE = LabelEncoder()\ndf_new['city_#'] = LE.fit_transform(df_new['city'])\ndf_new['category_name_#'] = LE.fit_transform(df_new['category_name'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# normalize these new columns: Replace original value by (original value)/(maximum value)\ndf_new['city_#'] = df_new['city_#']/df_new['city_#'].max()\ndf_new['category_name_#'] = df_new['category_name_#']/df_new['category_name_#'].max()\n\ndf_new.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 6.5) Binning"},{"metadata":{},"cell_type":"markdown","source":"<b>Why binning?</b>\n<p>\n    Binning is a process of transforming continuous numerical variables into discrete categorical 'bins', for grouped analysis.\n</p>\n\n<b>Example: </b>\n<p>In our dataset, \"distance (km)\" is a real valued variable ranging from 0.9 to 127.5, it has 159 unique values. What if we only care about the distances like close, medium and far (3 types)? Can we rearrange them into three ‘bins' to simplify analysis? </p>\n\n<p>We will use the Pandas method 'cut' to segment the 'distance (km)' column into 3 bins </p>\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"bins = np.linspace(min(df_new[\"distance (km)\"]), max(df_new[\"distance (km)\"]), 4)\ngroup_names = ['Close', 'Medium', 'Far']\ndf_new['distance'] = pd.cut(df_new['distance (km)'], bins, labels=group_names, include_lowest=True )\ndf_new.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%matplotlib inline\nimport matplotlib.pyplot as plt\nplt.bar(group_names, df_new[\"distance\"].value_counts())\n\n# set x/y labels and plot title\nplt.xlabel(\"distance\")\nplt.ylabel(\"count\")\nplt.title(\"distance bins\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 6.6) Indicator variable (or dummy variable)"},{"metadata":{},"cell_type":"markdown","source":"<b>What is an indicator variable?</b>\n<p>\n    An indicator variable (or dummy variable) is a numerical variable used to label categories. They are called 'dummies' because the numbers themselves don't have inherent meaning. \n</p>\n\n<b>Why we use indicator variables?</b>\n<p>\n    So we can use categorical variables for machine learning algorithms later in this project.\n</p>\n\n<b>Example</b>\n<p>\n    We see the column \"country_code\" has three unique values, \"DE\", \"NL\" or \"BE\". Most of the algorithms do not understand words, only numbers. To use this attribute in analysis, we convert \"country_code\" into indicator variables.\n</p>\n\n<p>\n    We will use the panda's method 'get_dummies' to assign numerical values to different columns shown below. \n</p>\n\n<p>\n    \"country_code\", \"state\", \"referralId\", \"label\", \"type\", \"distance\" "},{"metadata":{"trusted":true},"cell_type":"code","source":"dummy_variable_country_code = pd.get_dummies(df_new[\"country_code\"])\ndummy_variable_state = pd.get_dummies(df_new[\"state\"])\ndummy_variable_referralId = pd.get_dummies(df_new[\"referralId\"])\ndummy_variable_label = pd.get_dummies(df_new[\"label\"])\ndummy_variable_type = pd.get_dummies(df_new[\"type\"])\ndummy_variable_distance = pd.get_dummies(df_new[\"distance\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# merge dataframe \"df_new\" and dummy_variable dataframes\ndf_dummy = pd.concat([df_new, dummy_variable_country_code, dummy_variable_state, dummy_variable_referralId, dummy_variable_label, dummy_variable_type, dummy_variable_distance], axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# drop the original columns\ndf_dummy.drop(['country_code', 'state', 'referralId', 'label', 'type', 'distance'], axis = 1, inplace = True)\ndf_dummy.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# lets also drop some of the unnecessary columns \ndf_dummy.drop(['id', 'city', 'category_name', 'country', 'adress', 'name'], axis = 1, inplace = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_dummy.head(2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**change distance (km) name into distance_norm**"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_dummy.rename(columns = {'distance (km)': 'distance_norm'}, inplace = True)\ndf_dummy.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"markdown","source":"## 7) Exploratory Data Analysis\n* 7.1 Analyzing Individual Feature Patterns using Visualization\n* 7.2 Descriptive Statistical Analysis\n* 7.3 Basics of Grouping\n* 7.4 Correlation and Causation"},{"metadata":{"trusted":false},"cell_type":"markdown","source":"## 7.1) Analyzing Individual Feature Patterns using Visualization"},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline ","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"markdown","source":"<h4>How to choose the right visualization method?</h4>\n<p>When visualizing individual variables, it is important to first understand what type of variable you are dealing with. This will help us find the right visualization method for that variable.</p>\n\nfor example, we can calculate the correlation between variables  of type \"int64\" or \"float64\" using the method \"corr\":"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_new.dtypes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<h2>7.1.1) Continuous numerical variables:</h2> \n\n<p>Continuous numerical variables are variables that may contain any value within some range. Continuous numerical variables can have the type \"int64\" or \"float64\". A great way to visualize these variables is by using scatterplots with fitted lines.</p>\n\n<p>We can use \"regplot\", which plots the scatterplot plus the fitted regression line for the data.</p>"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_new.corr()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Letw plot heat map of the correlations**"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.heatmap(df_new.corr(), annot=True)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Here we see correlation between latitue and distance (km)**"},{"metadata":{},"cell_type":"markdown","source":"* The latitude is the ***north or south*** distance of a point on the earth's surface from the equator\n\n* The longitude describes one of the two coordinates of a place on the earth's surface, namely its position ***east or west*** of a defined north-south line, the prime meridian"},{"metadata":{"trusted":true},"cell_type":"code","source":"# latitude as potential segmenter \nfrom scipy import stats\na = df_new['latitute']\nb = df_new['distance (km)']\nprint('p values: ', stats.pearsonr(a, b)[1])\nsns.regplot(x=\"latitute\", y=\"distance (km)\", data=df_new)\nprint('Correlation: ', df_new[['latitute', 'distance (km)']].corr().iloc[0,1])\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# longitute as a potential segmenter \nc = df_new['longitute']\nb = df_new['distance (km)']\nprint('p values: ', stats.pearsonr(c, b)[1])\nsns.regplot(x=\"longitute\", y=\"distance (km)\", data=df_new)\nprint('Correlation: ', df_new[['longitute', 'distance (km)']].corr().iloc[0,1])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* We understand from above plots that there is a positive correlation between latitude vs distance and negative correlation between longitute vs distance (since p values for both cases are much low, that is p<0.001)\n* Latitue: North or south distance of a point\n* We can conclude from here that the venues are mostly distributed in the North - South line "},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.lmplot(x=\"latitute\", y=\"distance (km)\", hue = 'type', data=df_new)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**We see that the distribution of business and target market venues are almost paralel and in the N-S direction. Actually, when we \nexamine carefully the map depicted above, they are mostly distributed on the North of Aachen. Thus, latitute column will be a good indicator**"},{"metadata":{},"cell_type":"markdown","source":"<h3>7.1.2) Categorical variables</h3>\n\n<p>These are variables that describe a 'characteristic' of a data unit, and are selected from a small group of categories. The categorical variables can have the type \"object\" or \"int64\". A good way to visualize categorical variables is by using boxplots.</p>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# lets examine country_code and distance (km) \nsns.boxplot(x=\"country_code\", y=\"distance (km)\", data=df_new)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The distribution of distance between countries have a significant overlap, it does not give us much information. But we can say that the distribution of the venues in Germany is more in bulk  "},{"metadata":{"trusted":true},"cell_type":"code","source":"# lets examine referralId and distance (km)\n# we see that referralId could distinguish a little bit the distance of venues\nplt.figure(figsize = (9,5))\nsns.boxplot(x=\"referralId\", y=\"distance (km)\", data=df_new)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# lets examine label and distance (km)\n# we see that label may distinguish the distance of venues\nplt.figure(figsize = (15,7))\nsns.boxplot(x=\"label\", y=\"distance (km)\", data=df_new)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# lets examine type and distance (km)\n# we see that type may distinguish the distance of venues\n# business venues are a little farther from Aachen as compared to the target market\nsns.boxplot(x=\"type\", y=\"distance (km)\", data=df_new)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 7.2) Descriptive Statistical Analysis"},{"metadata":{},"cell_type":"markdown","source":"<p>Let's first take a look at the variables by utilizing a description method.</p>\n\n<p>The <b>describe</b> function automatically computes basic statistics for all continuous variables. Any NaN values are automatically skipped in these statistics.</p>\n\nThis will show:\n<ul>\n    <li>the count of that variable</li>\n    <li>the mean</li>\n    <li>the standard deviation (std)</li> \n    <li>the minimum value</li>\n    <li>the IQR (Interquartile Range: 25%, 50% and 75%)</li>\n    <li>the maximum value</li>\n<ul>\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_new.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We see that in the category_name column there are outliers\ndf_new.boxplot()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Value Counts**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# referralId\ndf_referralId = df_new.referralId.value_counts().to_frame()\ndf_referralId.rename(columns = {'referralId': 'Count'}, inplace = True)\ndf_referralId.index.name = 'referralId'\ndf_referralId","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# referralId bar plot\ndf_referralId = df_new.referralId.value_counts().to_frame()\nsns.barplot(x=df_referralId.index, y=\"referralId\", data=df_referralId)\nplt.xlabel('referralId')\nplt.ylabel('Count')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# country_code\ndf_country_code = df_new.country_code.value_counts().to_frame()\nsns.barplot(x=df_country_code.index, y=\"country_code\", data=df_country_code)\nplt.xlabel('Country')\nplt.ylabel('Count')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# city\ndf_city = df_new.city.value_counts().to_frame()\nplt.figure(figsize = (18,8))\nsns.barplot(x=df_city.index, y=\"city\", data=df_city)\nplt.xticks(rotation = 90)\nplt.ylabel('Count')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# label\ndf_label = df_new.label.value_counts().to_frame()\nplt.figure(figsize = (18,8))\nplt.xticks(rotation = 90)\nsns.barplot(x=df_label.index, y=\"label\", data=df_label)\nplt.ylabel('Count')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# type\ndf_type = df_new.type.value_counts().to_frame()\nsns.barplot(x=df_type.index, y=\"type\", data=df_type)\nplt.xticks(rotation = 90)\nplt.ylabel('Count')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 7.3) Grouping"},{"metadata":{},"cell_type":"markdown","source":"<p>The \"groupby\" method groups data by different categories. The data is grouped based on one or several variables and analysis is performed on the individual groups.</p>"},{"metadata":{},"cell_type":"markdown","source":"### 7.3.1) Group by country & type and observe the distance"},{"metadata":{"trusted":true},"cell_type":"code","source":"# grouping results\ndf_group = df_new[['referralId','country_code', 'city', 'label', 'type', 'distance (km)']]\ngrouped_test1 = df_group.groupby(['country_code','type'],as_index=False).mean()\ngrouped_test1","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<p>This grouped data is much easier to visualize when it is made into a pivot table. A pivot table is like an Excel spreadsheet, with one variable along the column and another along the row. We can convert the dataframe to a pivot table using the method \"pivot \" to create a pivot table from the groups.</p>\n\n<p>In this case, we will leave the \"country_code\" variable as the rows of the table, and pivot \"type\" to construct the columns of the table:</p>"},{"metadata":{"trusted":true},"cell_type":"code","source":"grouped_pivot = grouped_test1.pivot(index='country_code', columns='type')\ngrouped_pivot","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's use a heat map to visualize the relationship between country_code vs distance."},{"metadata":{"trusted":true},"cell_type":"code","source":"#use the grouped results\nfig, ax = plt.subplots()\nim = ax.pcolor(grouped_pivot, cmap='RdBu')\n\n#label names\nrow_labels = grouped_pivot.columns.levels[1]\ncol_labels = grouped_pivot.index\n\n#move ticks and labels to the center\nax.set_xticks(np.arange(grouped_pivot.shape[1]) + 0.5, minor=False)\nax.set_yticks(np.arange(grouped_pivot.shape[0]) + 0.5, minor=False)\n\n#insert labels\nax.set_xticklabels(row_labels, minor=False)\nax.set_yticklabels(col_labels, minor=False)\n\n#rotate label if too long\nplt.xticks(rotation=0)\n\nfig.colorbar(im)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Target market venues in Belgium are the closest ones to Aachen. On the other hand, business venues in Belgium are the fathest\n* In Germany, both the business and target market venues are almost at the same distance\n* In Netherlands, target market venues are closer as compared to the business venues"},{"metadata":{},"cell_type":"markdown","source":"**We can say that it might make sense to open a business in Belgium both near to Netherlands and Aachen. But we need further analysis**"},{"metadata":{},"cell_type":"markdown","source":"### 7.3.2) Group by country & label and observe the distance"},{"metadata":{"trusted":true},"cell_type":"code","source":"grouped_test2 = df_group.groupby(['country_code','label'],as_index=False).mean()\ngrouped_pivot = grouped_test2.pivot(index='country_code', columns='label')\n\n#use the grouped results\nfig, ax = plt.subplots(figsize=(20, 5))\nim = ax.pcolor(grouped_pivot, cmap='RdBu')\n\n#label names\nrow_labels = grouped_pivot.columns.levels[1]\ncol_labels = grouped_pivot.index\n\n#move ticks and labels to the center\nax.set_xticks(np.arange(grouped_pivot.shape[1]) + 0.5, minor=False)\nax.set_yticks(np.arange(grouped_pivot.shape[0]) + 0.5, minor=False)\n\n#insert labels\nax.set_xticklabels(row_labels, minor=False)\nax.set_yticklabels(col_labels, minor=False)\n\n#rotate label if too long\nplt.xticks(rotation=0)\n\nfig.colorbar(im)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Genetic hospitals and genetic diagnosis in Belgium and life science venues in Netherlands are close to Aachen. These results can affect our decisions deeply.   "},{"metadata":{"trusted":true},"cell_type":"code","source":"# We see that in the category_name_# column there are outliers in the target market\ndf_new.boxplot(by='type', figsize=(15,5), layout=(1,5))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 7.4) Correlation and Causation"},{"metadata":{},"cell_type":"markdown","source":"<p><b>Correlation</b>: a measure of the extent of interdependence between variables.</p>\n\n<p><b>Causation</b>: the relationship between cause and effect between two variables.</p>\n\n<p>It is important to know the difference between these two and that correlation does not imply causation. Determining correlation is much simpler  than determining causation as causation may require independent experimentation.</p>"},{"metadata":{},"cell_type":"markdown","source":"<p3>Pearson Correlation</p>\n<p>The Pearson Correlation measures the linear dependence between two variables X and Y.</p>\n<p>The resulting coefficient is a value between -1 and 1 inclusive, where:</p>\n<ul>\n    <li><b>1</b>: Total positive linear correlation.</li>\n    <li><b>0</b>: No linear correlation, the two variables most likely do not affect each other.</li>\n    <li><b>-1</b>: Total negative linear correlation.</li>\n</ul>"},{"metadata":{},"cell_type":"markdown","source":"<p>Pearson Correlation is the default method of the function \"corr\".  Like before we can calculate the Pearson Correlation of the of the 'int64' or 'float64'  variables.</p>"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_new.corr()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" Sometimes we would like to know the significant of the correlation estimate. How?\n \n <b>P-value</b>: \n<p>What is this P-value? The P-value is the probability value that the correlation between these two variables is statistically significant. Normally, we choose a significance level of 0.05, which means that we are 95% confident that the correlation between the variables is significant.</p>\n\nBy convention, when the\n<ul>\n    <li>p-value is $<$ 0.001: we say there is strong evidence that the correlation is significant.</li>\n    <li>the p-value is $<$ 0.05: there is moderate evidence that the correlation is significant.</li>\n    <li>the p-value is $<$ 0.1: there is weak evidence that the correlation is significant.</li>\n    <li>the p-value is $>$ 0.1: there is no evidence that the correlation is significant.</li>\n</ul>"},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy import stats","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**7.4.1) Latitute vs Distance**"},{"metadata":{"trusted":true},"cell_type":"code","source":"pearson_coef, p_value = stats.pearsonr(df_new['latitute'], df_new['distance (km)'])\nprint(\"The Pearson Correlation Coefficient is\", pearson_coef, \" with a P-value of P =\", p_value) \nprint('p value is smaller than 0.001: ', p_value < 0.001)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"p_value < 0.001 implies that the correlation is statistically significant, although the linear relationship isn't extremely strong with correlation coefficient = 0.5216026202606111"},{"metadata":{},"cell_type":"markdown","source":"**7.4.2) Longitute vs Distance**"},{"metadata":{"trusted":true},"cell_type":"code","source":"pearson_coef, p_value = stats.pearsonr(df_new['longitute'], df_new['distance (km)'])\nprint(\"The Pearson Correlation Coefficient is\", pearson_coef, \" with a P-value of P =\", p_value) \nprint('p value is smaller than 0.001: ', p_value < 0.001)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"p_value < 0.001 implies that the correlation is statistically significant, although the linear relationship isn't strong with correlation coefficient = -0.2875019558678858\n\n(In other words, we have a week relation but we can trust this relation since it has very small p value)\n\nThe distance of venues from Aachen increases as we go on latitute and slightly decreases as we go on longitute. "},{"metadata":{},"cell_type":"markdown","source":"**7.4.3) City_# vs Distance**"},{"metadata":{"trusted":true},"cell_type":"code","source":"pearson_coef, p_value = stats.pearsonr(df_new['city_#'], df_new['distance (km)'])\nprint(\"The Pearson Correlation Coefficient is\", pearson_coef, \" with a P-value of P =\", p_value) \nprint('p value is higher than 0.1: ', p_value > 0.1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"No relation between city_# and distance"},{"metadata":{},"cell_type":"markdown","source":"### 7.5) ANOVA: Analysis of Variance"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"<p>The Analysis of Variance  (ANOVA) is a statistical method used to test whether there are significant differences between the means of two or more groups. ANOVA returns two parameters:</p>\n\n<p><b>F-test score</b>: ANOVA assumes the means of all groups are the same, calculates how much the actual means deviate from the assumption, and reports it as the F-test score. A larger score means there is a larger difference between the means.</p>\n\n<p><b>P-value</b>:  P-value tells how statistically significant is our calculated score value.</p>\n\n<p>If a variable is strongly correlated with another variable that we are analyzing, expect ANOVA to return a sizeable F-test score and a small p-value.</p>"},{"metadata":{},"cell_type":"markdown","source":"<p>Since ANOVA analyzes the difference between different groups of the same variable, the groupby function will come in handy. Because the ANOVA algorithm averages the data automatically, we do not need to take the average before hand.</p>"},{"metadata":{},"cell_type":"markdown","source":"### 7.5.1) referralId vs distance"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_categorical = df_new[['referralId', 'country_code', 'type', 'label', 'distance (km)']]\ndf_categorical_grouped1 = df_categorical[['referralId', 'distance (km)']].groupby(['referralId']) \ndf_categorical_grouped1.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"refID = []\nnum_of_refID = len(df_categorical.referralId.value_counts().index)\nfor i in range (0,num_of_refID):\n    refID.append(df_categorical.referralId.value_counts().index[i])\n\n# ANOVA of all groups\nf_val, p_val = stats.f_oneway(df_categorical_grouped1.get_group(refID[0])['distance (km)'], df_categorical_grouped1.get_group(refID[1])['distance (km)'], df_categorical_grouped1.get_group(refID[2])['distance (km)'], df_categorical_grouped1.get_group(refID[3])['distance (km)'])  \nprint(\"ANOVA results for all groups in referralId: F=\", f_val, \", P =\", p_val)\n\nfor i in range (0, num_of_refID):\n    for j in range (i+1, num_of_refID):\n        f_val, p_val = stats.f_oneway(df_categorical_grouped1.get_group(refID[i])['distance (km)'], df_categorical_grouped1.get_group(refID[j])['distance (km)'])  \n        print(\"ANOVA results for referralId {0} and {1}: F= {2}, P = {3}\".format(refID[i], refID[j], f_val, p_val))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For high p_value (> 0.1) and low F score, we can not say a statistically significant results\nBut for other referralIds, we can say that there is a differance and they are statistically significant (p_values are low (<0.001) and F scores are high) "},{"metadata":{},"cell_type":"markdown","source":"### 7.5.2) Country vs Distance"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_categorical_grouped2 = df_categorical[['country_code', 'distance (km)']].groupby(['country_code']) \ndf_categorical_grouped2.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ANOVA of all groups\nf_val, p_val = stats.f_oneway(df_categorical_grouped2.get_group('NL')['distance (km)'], df_categorical_grouped2.get_group('BE')['distance (km)'], df_categorical_grouped2.get_group('DE')['distance (km)'])  \nprint( \"ANOVA results for all groups in country_code: F=\", f_val, \", P =\", p_val)\n\n# ANOVA of group with country_code NL and BE \nf_val, p_val = stats.f_oneway(df_categorical_grouped2.get_group('NL')['distance (km)'], df_categorical_grouped2.get_group('BE')['distance (km)'])  \nprint( \"ANOVA results for NL and BE: F=\", f_val, \", P =\", p_val) \n\n# ANOVA of group with country_code NL and DE \nf_val, p_val = stats.f_oneway(df_categorical_grouped2.get_group('NL')['distance (km)'], df_categorical_grouped2.get_group('DE')['distance (km)'])  \nprint( \"ANOVA results for NL and DE: F=\", f_val, \", P =\", p_val)   \n\n# ANOVA of group with country_code BE and DE \nf_val, p_val = stats.f_oneway(df_categorical_grouped2.get_group('BE')['distance (km)'], df_categorical_grouped2.get_group('DE')['distance (km)'])  \nprint( \"ANOVA results for BE and DE: F=\", f_val, \", P =\", p_val)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can mention that BE and DE have slight differance and it may be statistically significant. But no relation for NL and BE"},{"metadata":{},"cell_type":"markdown","source":"### 7.5.3) Type vs Distance"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_categorical_grouped3 = df_categorical[['type', 'distance (km)']].groupby(['type']) \ndf_categorical_grouped3.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ANOVA of all groups\nf_val, p_val = stats.f_oneway(df_categorical_grouped3.get_group('business')['distance (km)'], df_categorical_grouped3.get_group('target market')['distance (km)'])  \nprint( \"ANOVA results for all groups in type: F=\", f_val, \", P =\", p_val)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can mention that business and target market are different groups"},{"metadata":{},"cell_type":"markdown","source":"### 8) Model Development"},{"metadata":{},"cell_type":"markdown","source":"We want to predict the possible location of the venue to be build up. To achieve this goal, we need to predict latitute and longitute, which are continous variables. Target feature is not a labeled data, so we can not use supervised learning algorithms. For continous variables we can choose regression models such as linear regression and polynomial regression models. We are also going to build unsupervised learning models to seek any reasonable relationship   "},{"metadata":{"trusted":true},"cell_type":"code","source":"df_dummy.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_lat = df_dummy.drop(['latitute'], axis = 1)\ny_lat = df_dummy['latitute']\n\nx_lon = df_dummy.drop(['longitute'], axis = 1)\ny_lon = df_dummy['longitute']\n\nfrom sklearn.model_selection import train_test_split\nx_train_lat, x_test_lat, y_train_lat, y_test_lat = train_test_split(x_lat, y_lat, test_size=0.15, random_state=1)\nx_train_lon, x_test_lon, y_train_lon, y_test_lon = train_test_split(x_lon, y_lon, test_size=0.15, random_state=1)\n\nprint(\"number of test samples :\", x_test_lat.shape[0])\nprint(\"number of training samples:\",x_train_lat.shape[0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 8.1) Linear Regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nlr_lat = LinearRegression()\nlr_lat.fit(x_train_lat, y_train_lat)\n\nprint('Linear Regression for Latitute Prediction')\nprint('train data prediction score: ',lr_lat.score(x_train_lat, y_train_lat))\nprint('test data prediction score: ',lr_lat.score(x_test_lat, y_test_lat))\n\nlr_lon = LinearRegression()\nlr_lon.fit(x_train_lon, y_train_lon)\n\nprint()\nprint('Linear Regression for Longitute Prediction')\nprint('train data prediction score: ',lr_lon.score(x_train_lon, y_train_lon))\nprint('test data prediction score: ',lr_lon.score(x_test_lon, y_test_lon))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Train data prediction score is high compared to the test data prediction score. This is actually expected result. Nevertheless, the results show that the model makes good predicitons"},{"metadata":{},"cell_type":"markdown","source":"**Cross-validation Score**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# The parameter 'cv' determines the number of folds. \n# We divide data into cv = 4 parts and use 1 portion for testing and 3 portion for training. \n# Then we change the order and perform the procedure again \nfrom sklearn.model_selection import cross_val_score\nRcross_lat = abs(cross_val_score(lr_lat, x_lat, y_lat, cv=4))\nRcross_lon = abs(cross_val_score(lr_lon, x_lon, y_lon, cv=4))\n\nprint('Cross validation scores for latitute: ', Rcross_lat)\nprint('Cross validation scores for longitute: ', Rcross_lon)\nprint(\"\")\nprint(\"Latitute: The mean of the folds: \", Rcross_lat.mean(), \"and the standard deviation: \" , Rcross_lat.std())\nprint(\"Longitute: The mean of the folds: \", Rcross_lon.mean(), \"and the standard deviation: \" , Rcross_lon.std())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- We see that when we change the order of input data, predictions for latitute and for longitute slightly decreases\n- But for longitute, it is more reliable, mean is higher and standart deviation is lower. "},{"metadata":{},"cell_type":"markdown","source":"### 8.2) Polynomial Regression and Pipelines"},{"metadata":{},"cell_type":"markdown","source":"<p>Data Pipelines simplify the steps of processing the data. We use the module <b>Pipeline</b> to create a pipeline. We also use <b>StandardScaler</b> as a step in our pipeline.</p>\n\nWe create the pipeline, by creating a list of tuples including the name of the model or estimator and its corresponding constructor."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import r2_score\n\nfor i in range (1, 5):\n    Input=[('scale',StandardScaler()), ('polynomial', PolynomialFeatures(degree=i)), ('model',LinearRegression())]\n    pipe=Pipeline(Input)\n\n    pipe_lat = pipe.fit(x_train_lat, y_train_lat)\n    y_train_lat_hat = pipe_lat.predict(x_train_lat)\n    y_test_lat_hat = pipe_lat.predict(x_test_lat)\n\n    pipe_lon = pipe.fit(x_train_lon, y_train_lon)\n    y_train_lon_hat = pipe_lon.predict(x_train_lon)\n    y_test_lon_hat = pipe_lon.predict(x_test_lon)\n    \n    print('Degree: ', i)\n    print('')\n    print('Polynomial Regression for Latitute Prediction')\n    print('train data prediction score: ',r2_score(y_train_lat, y_train_lat_hat))\n    print('test data prediction score: ',r2_score(y_test_lat, y_test_lat_hat))\n\n    print()\n    print('Polynomial Regression for Longitute Prediction')\n    print('train data prediction score: ',r2_score(y_train_lon, y_train_lon_hat))\n    print('test data prediction score: ',r2_score(y_test_lon, y_test_lon_hat))\n    print()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**For degree = 1, prediction scores are good. But for degree > 1, we see that prediction score of train data is very high, however, prediction score of test data is very low. This implies that our model is overfitted, it does not give nice results for degree > 1**"},{"metadata":{},"cell_type":"markdown","source":"### 8.3) K-Means Clustering"},{"metadata":{},"cell_type":"markdown","source":"The KMeans class has many parameters that can be used, but we will be using these three:\n<ul>\n    <li> <b>init</b>: Initialization method of the centroids. </li>\n    <ul>\n        <li> Value will be: \"k-means++\" </li>\n        <li> k-means++: Selects initial cluster centers for k-mean clustering in a smart way to speed up convergence.</li>\n    </ul>\n    <li> <b>n_clusters</b>: The number of clusters to form as well as the number of centroids to generate. </li>\n    <li> <b>n_init</b>: Number of time the k-means algorithm will be run with different centroid seeds. The final results will be the best output of n_init consecutive runs in terms of inertia. </li>\n    <ul> <li> Value will be: 12 </li> </ul>\n</ul>\n\nInitialize KMeans with these parameters, where the output parameter is called <b>k_means</b>."},{"metadata":{"trusted":true},"cell_type":"code","source":"# we want to cluster the data into 4 groups\nX = df_dummy.values\nfrom sklearn.cluster import KMeans \nk_means = KMeans(init = \"k-means++\", n_clusters = 4, n_init = 12)\nk_means.fit(X)\nk_means_labels = k_means.labels_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_kmeans = pd.concat([df_all, pd.DataFrame(k_means_labels, columns = ['kmeans_label'])], axis = 1)\ndf_kmeans.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"map_kmeans = folium.Map(location=[latitude, longitude], zoom_start=7) # generate map centred around Aachen\n\n# add the group venues that are nearby as blue circle markers\nfor lat, lng, label in zip(df_kmeans['latitute'], df_kmeans['longitute'], df_kmeans['kmeans_label']):\n    \n    if label == 0.0:\n        label_color = 'red'\n    elif label == 1.0:\n        label_color = 'green'\n    elif label == 2.0:\n        label_color = 'blue'\n    else:\n        label_color = 'purple'\n    \n    folium.vector_layers.CircleMarker(\n        [lat, lng],\n        radius=5,\n        color=label_color,\n        popup=label,\n        fill = True,\n        fill_color=label_color,\n        fill_opacity=0.6\n    ).add_to(map_kmeans)\n    \nmap_kmeans","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_kmeans_0 = df_kmeans[df_kmeans.kmeans_label == 0.0][['name', 'country_code', 'city', 'label', 'type']]\ndf_kmeans_1 = df_kmeans[df_kmeans.kmeans_label == 1.0][['name', 'country_code', 'city', 'label', 'type']]\ndf_kmeans_2 = df_kmeans[df_kmeans.kmeans_label == 2.0][['name', 'country_code', 'city', 'label', 'type']]\ndf_kmeans_3 = df_kmeans[df_kmeans.kmeans_label == 3.0][['name', 'country_code', 'city', 'label', 'type']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_kmeans_0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_kmeans_1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_kmeans_2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_kmeans_3","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 9. Results\n- There are only 4 business venues within 50km\n- We understand from above plots that there is a positive correlation between latitude vs distance and negative correlation between longitute vs distance (since p values for both cases are much low, that is p<0.001)\n- The distribution of distance between countries have a significant overlap, it does not give us much information. But we can say that the distribution of the venues in Germany is more in bulk\n- Target market venues in Belgium are the closest ones to Aachen. On the other hand, business venues in Belgium are the fathest\n- In Germany, both the business and target market venues are almost at the same distance\n- In Netherlands, target market venues are closer as compared to the business venues\n- We can mention that BE and DE have slight differance and it may be statistically significant. But no relation for NL and BE\n- We can mention that business and target market are different groups"},{"metadata":{},"cell_type":"markdown","source":"### 10. Discussion\n\n- We see that the distribution of business and target market venues are almost paralel and in the N-S direction. Actually, when we examine carefully the map depicted above, they are mostly distributed on the North of Aachen. Thus, latitute column will be a good indicator. Moreover, business venues are farther than target market venues\n- Genetic hospitals and genetic diagnosis in Belgium and life science venues in Netherlands are close to Aachen. So, the target is this venues we should take these results into accout\n- We want to setup a business. The results show that, the venue should be in one of the df_kmeans#. Interestingly, almost all business venues are in the same group. This shows business venues are smartly distributed and we should also consider this when choosing the location"},{"metadata":{},"cell_type":"markdown","source":"### 11. Conclusion\n- We can conclude from here that the venues are mostly distributed in the North - South line. There is a positive correlation with latitute and negative correlation with longitute.  \n- We can say that it might make sense to open a business in Belgium both near to Netherlands and Aachen. Specifically, if the target is genetic hospitals and genetic diagnosis centers and life science venues. \n- We can say that the distribution of the venues in Germany is more in bulk. This shows that, it may not be a good choise to invest in Germany especially if you are a startup company. "}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}